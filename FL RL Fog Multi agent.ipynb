{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e095d3c1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e8b6fe72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.19.0\n",
      "Eager execution enabled: True\n",
      "No GPUs found, using CPU\n",
      "Setup complete!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import logging\n",
    "from typing import Dict, List, Tuple, Any\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "import pickle\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Kiểm tra TensorFlow\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"Eager execution enabled: {tf.executing_eagerly()}\")\n",
    "\n",
    "# Đảm bảo eager execution được bật để debug dễ dàng\n",
    "tf.config.run_functions_eagerly(True)\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    print(f\"Found {len(gpus)} GPU(s)\")\n",
    "    for gpu in gpus:\n",
    "        print(f\"  - {gpu}\")\n",
    "    # Đặt memory growth để tránh OOM\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "else:\n",
    "    print(\"No GPUs found, using CPU\")\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d486b31b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "b6afbe85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-21 13:31:06,569 - INFO - Directories created successfully at: /Users/macbook/Desktop/FL-RL-Dos detection/Multi agents RL-FL-Fog/Multi agents RL-FL-Fog\n",
      "2025-05-21 13:31:06,570 - INFO - Configuration validated successfully. Data at: /Users/macbook/Desktop/FL-RL-Dos detection/Multi agents RL-FL-Fog/Multi agents RL-FL-Fog/data/Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config validated successfully!\n"
     ]
    }
   ],
   "source": [
    "class Config:\n",
    "    def __init__(self):\n",
    "        # Paths\n",
    "        self.BASE_DIR = \"/Users/macbook/Desktop/FL-RL-Dos detection/Multi agents RL-FL-Fog/Multi agents RL-FL-Fog\"\n",
    "        self.DATA_DIR = os.path.join(self.BASE_DIR, \"data\")\n",
    "        self.DATA_PATH = os.path.join(self.DATA_DIR, \"Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv\")\n",
    "        self.CHECKPOINT_DIR = os.path.join(self.BASE_DIR, \"checkpoints\")\n",
    "        self.RESULTS_DIR = os.path.join(self.BASE_DIR, \"results\")\n",
    "        \n",
    "        # Metadata\n",
    "        self.TIMESTAMP = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        self.USER = os.getenv(\"USER\", \"unknown\")\n",
    "        \n",
    "        # Selected Features - Sử dụng tên cột chính xác từ dữ liệu\n",
    "        self.SELECTED_FEATURES = [\n",
    "            ' Source Port',\n",
    "            ' Destination Port',\n",
    "            ' Protocol',\n",
    "            ' Flow Duration',\n",
    "            ' Total Fwd Packets',\n",
    "            ' Total Backward Packets',\n",
    "            'Total Length of Fwd Packets',\n",
    "            ' Total Length of Bwd Packets',\n",
    "            ' Fwd Packet Length Max',\n",
    "            ' Fwd Packet Length Min',\n",
    "            ' Fwd Packet Length Mean',\n",
    "            ' Fwd Packet Length Std',\n",
    "            'Bwd Packet Length Max'\n",
    "        ]\n",
    "        \n",
    "        # Tên cột nhãn\n",
    "        self.LABEL_COLUMN = \" Label\"\n",
    "        \n",
    "        # Network Architecture\n",
    "        self.NUM_FOG_NODES = 2  # Giảm xuống tránh OOM\n",
    "        self.NUM_AGENTS_PER_NODE = 2  # Giảm xuống tránh OOM\n",
    "        self.STATE_DIM = len(self.SELECTED_FEATURES)\n",
    "        self.ACTION_DIM = 2  # BENIGN và DDoS\n",
    "        self.HIDDEN_UNITS = [64, 32]  # Giảm kích thước mạng\n",
    "        \n",
    "        # Training Parameters\n",
    "        self.BATCH_SIZE = 32  # Giảm batch size\n",
    "        self.LEARNING_RATE = 0.001\n",
    "        self.NUM_EPISODES = 10  # Giảm số episodes\n",
    "        self.UPDATE_INTERVAL = 5\n",
    "        self.GAMMA = 0.99\n",
    "        self.WINDOW_SIZE = 20  # Giảm window size để tránh OOM\n",
    "        \n",
    "        # Federated Learning\n",
    "        self.FL_ROUNDS = 2  # Ít vòng để test nhanh\n",
    "        self.LOCAL_EPOCHS = 1\n",
    "        \n",
    "        # PPO Parameters\n",
    "        self.PPO_EPOCHS = 2  # Giảm số epochs\n",
    "        self.PPO_BATCH_SIZE = 32  # Giảm batch size\n",
    "        self.PPO_CLIP_RATIO = 0.2\n",
    "        self.PPO_TARGET_KL = 0.01\n",
    "        \n",
    "        # Data Distribution\n",
    "        self.DATA_DISTRIBUTION_TYPE = 'equal'  # Phân phối đồng đều để đơn giản hóa\n",
    "        self.DIRICHLET_ALPHA = 0.5\n",
    "        \n",
    "        # Early Stopping & LR Scheduling\n",
    "        self.EARLY_STOPPING_PATIENCE = 3\n",
    "        self.EARLY_STOPPING_MIN_DELTA = 0.001\n",
    "        self.LR_DECAY_STEPS = 5\n",
    "        self.LR_DECAY_RATE = 0.9\n",
    "        self.LR_WARMUP_STEPS = 1\n",
    "        self.LR_MIN = 1e-6\n",
    "        self.LR_STRATEGY = 'exponential'\n",
    "        self.CHECKPOINT_FREQUENCY = 1\n",
    "        \n",
    "        # Data Processing Options\n",
    "        self.USE_DATA_SAMPLING = True  # Lấy mẫu dữ liệu để tránh OOM\n",
    "        self.MAX_SAMPLES = 10000  # Giới hạn số mẫu để test nhanh\n",
    "        self.MAX_SAMPLES_PER_NODE = 3000  # Số lượng mẫu tối đa mỗi node\n",
    "        self.USE_MINI_BATCH = True  # Sử dụng mini-batch training\n",
    "        \n",
    "        # Create directories\n",
    "        self._create_directories()\n",
    "    \n",
    "    def _create_directories(self):\n",
    "        os.makedirs(self.DATA_DIR, exist_ok=True)\n",
    "        os.makedirs(self.CHECKPOINT_DIR, exist_ok=True)\n",
    "        os.makedirs(self.RESULTS_DIR, exist_ok=True)\n",
    "        logger.info(f\"Directories created successfully at: {self.BASE_DIR}\")\n",
    "    \n",
    "    def validate_config(self):\n",
    "        try:\n",
    "            # Check if data file exists\n",
    "            if not os.path.exists(self.DATA_PATH):\n",
    "                raise FileNotFoundError(f\"Data file not found at {self.DATA_PATH}\")\n",
    "                \n",
    "            # Validate parameters\n",
    "            if self.BATCH_SIZE <= 0 or self.LEARNING_RATE <= 0 or self.WINDOW_SIZE <= 0:\n",
    "                raise ValueError(\"Invalid parameter values\")\n",
    "                \n",
    "            logger.info(f\"Configuration validated successfully. Data at: {self.DATA_PATH}\")\n",
    "            return True\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Configuration validation failed: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "# Kiểm tra cấu hình\n",
    "config = Config()\n",
    "try:\n",
    "    config.validate_config()\n",
    "    print(\"Config validated successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Config validation failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc0f4eb",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "2f90c95c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actor output shape: (5, 2)\n",
      "Critic output shape: (5, 1)\n",
      "Neural networks tested successfully!\n"
     ]
    }
   ],
   "source": [
    "class ActorNetwork(tf.keras.Model):\n",
    "    def __init__(self, state_dim: int, action_dim: int, hidden_units: List[int]):\n",
    "        super(ActorNetwork, self).__init__(name='actor_network')\n",
    "        \n",
    "        # Lớp LSTM để xử lý dữ liệu chuỗi thời gian\n",
    "        self.lstm = tf.keras.layers.LSTM(64, return_sequences=False, name='actor_lstm')\n",
    "        \n",
    "        self.hidden_layers = []\n",
    "        for i, units in enumerate(hidden_units):\n",
    "            self.hidden_layers.append(tf.keras.layers.Dense(\n",
    "                units, \n",
    "                activation='relu',\n",
    "                kernel_initializer='he_normal',\n",
    "                name=f'actor_dense_{i}'\n",
    "            ))\n",
    "            \n",
    "        self.output_layer = tf.keras.layers.Dense(\n",
    "            action_dim,\n",
    "            activation='softmax',\n",
    "            kernel_initializer='he_normal',\n",
    "            name='actor_output'\n",
    "        )\n",
    "        \n",
    "    def call(self, state, training=False):\n",
    "        # Đảm bảo đầu vào là tensor 3D (batch, time_steps, features)\n",
    "        # Sử dụng shape.rank thay vì len() để tránh lỗi với tensor symbolics\n",
    "        if tf.is_tensor(state) and state.shape.rank < 3:\n",
    "            # Nếu chỉ có 2D, thêm time_steps dimension\n",
    "            state = tf.expand_dims(state, axis=1)\n",
    "        elif not tf.is_tensor(state) and len(state.shape) < 3:\n",
    "            # Nếu là numpy array, kiểm tra shape\n",
    "            state = np.expand_dims(state, axis=1)\n",
    "            state = tf.convert_to_tensor(state, dtype=tf.float32)\n",
    "            \n",
    "        # Xử lý dữ liệu 3D qua LSTM\n",
    "        x = self.lstm(state, training=training)\n",
    "        \n",
    "        for layer in self.hidden_layers:\n",
    "            x = layer(x, training=training)\n",
    "            \n",
    "        return self.output_layer(x, training=training)\n",
    "    \n",
    "    def build_model(self, input_shape):\n",
    "        \"\"\"Build model với input shape cụ thể\"\"\"\n",
    "        inputs = tf.keras.Input(shape=input_shape)\n",
    "        outputs = self.call(inputs)\n",
    "        return tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "class CriticNetwork(tf.keras.Model):\n",
    "    def __init__(self, state_dim: int, hidden_units: List[int]):\n",
    "        super(CriticNetwork, self).__init__(name='critic_network')\n",
    "        \n",
    "        # Lớp LSTM để xử lý dữ liệu chuỗi thời gian\n",
    "        self.lstm = tf.keras.layers.LSTM(64, return_sequences=False, name='critic_lstm')\n",
    "        \n",
    "        self.hidden_layers = []\n",
    "        for i, units in enumerate(hidden_units):\n",
    "            self.hidden_layers.append(tf.keras.layers.Dense(\n",
    "                units, \n",
    "                activation='relu',\n",
    "                kernel_initializer='he_normal',\n",
    "                name=f'critic_dense_{i}'\n",
    "            ))\n",
    "            \n",
    "        self.output_layer = tf.keras.layers.Dense(\n",
    "            1,\n",
    "            kernel_initializer='he_normal',\n",
    "            name='critic_output'\n",
    "        )\n",
    "        \n",
    "    def call(self, state, training=False):\n",
    "        # Đảm bảo đầu vào là tensor 3D (batch, time_steps, features)\n",
    "        # Sử dụng shape.rank thay vì len() để tránh lỗi với tensor symbolics\n",
    "        if tf.is_tensor(state) and state.shape.rank < 3:\n",
    "            # Nếu chỉ có 2D, thêm time_steps dimension\n",
    "            state = tf.expand_dims(state, axis=1)\n",
    "        elif not tf.is_tensor(state) and len(state.shape) < 3:\n",
    "            # Nếu là numpy array, kiểm tra shape\n",
    "            state = np.expand_dims(state, axis=1)\n",
    "            state = tf.convert_to_tensor(state, dtype=tf.float32)\n",
    "            \n",
    "        # Xử lý dữ liệu 3D qua LSTM\n",
    "        x = self.lstm(state, training=training)\n",
    "        \n",
    "        for layer in self.hidden_layers:\n",
    "            x = layer(x, training=training)\n",
    "            \n",
    "        return self.output_layer(x, training=training)\n",
    "    \n",
    "    def build_model(self, input_shape):\n",
    "        \"\"\"Build model với input shape cụ thể\"\"\"\n",
    "        inputs = tf.keras.Input(shape=input_shape)\n",
    "        outputs = self.call(inputs)\n",
    "        return tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Kiểm tra các lớp mạng\n",
    "try:\n",
    "    # Tạo dữ liệu giả\n",
    "    batch_size, time_steps, features = 5, config.WINDOW_SIZE, config.STATE_DIM\n",
    "    dummy_input = tf.random.normal((batch_size, time_steps, features))\n",
    "    \n",
    "    # Tạo và test actor network\n",
    "    actor = ActorNetwork(features, config.ACTION_DIM, config.HIDDEN_UNITS)\n",
    "    actor_output = actor(dummy_input)\n",
    "    print(f\"Actor output shape: {actor_output.shape}\")\n",
    "    \n",
    "    # Tạo và test critic network\n",
    "    critic = CriticNetwork(features, config.HIDDEN_UNITS)\n",
    "    critic_output = critic(dummy_input)\n",
    "    print(f\"Critic output shape: {critic_output.shape}\")\n",
    "    \n",
    "    print(\"Neural networks tested successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Neural networks test failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c4431a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "00c93e53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled batch shapes - states: (5, 20, 13), actions: (5,)\n",
      "Iter 0: value=0.50, stop=False\n",
      "Iter 1: value=0.60, stop=False\n",
      "Iter 2: value=0.60, stop=False\n",
      "Iter 3: value=0.80, stop=False\n",
      "Iter 4: value=0.90, stop=False\n",
      "Step 1: LR = 0.000979\n",
      "Step 2: LR = 0.000959\n",
      "Step 3: LR = 0.000939\n",
      "Step 4: LR = 0.000919\n",
      "Step 5: LR = 0.000900\n",
      "Step 6: LR = 0.000881\n",
      "Step 7: LR = 0.000863\n",
      "Step 8: LR = 0.000845\n",
      "Step 9: LR = 0.000827\n",
      "Step 10: LR = 0.000810\n",
      "Utilities tested successfully!\n"
     ]
    }
   ],
   "source": [
    "class MemoryBuffer:\n",
    "    def __init__(self, capacity=10000):\n",
    "        self.capacity = capacity\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.next_states = []\n",
    "        self.size = 0\n",
    "        \n",
    "    def add(self, state, action, reward, next_state):\n",
    "        if self.size < self.capacity:\n",
    "            self.states.append(state)\n",
    "            self.actions.append(action)\n",
    "            self.rewards.append(reward)\n",
    "            self.next_states.append(next_state)\n",
    "            self.size += 1\n",
    "        else:\n",
    "            # Thay thế random một mẫu nếu đầy\n",
    "            idx = np.random.randint(0, self.capacity)\n",
    "            self.states[idx] = state\n",
    "            self.actions[idx] = action\n",
    "            self.rewards[idx] = reward\n",
    "            self.next_states[idx] = next_state\n",
    "            \n",
    "    def sample(self, batch_size):\n",
    "        if self.size < batch_size:\n",
    "            indices = range(self.size)\n",
    "        else:\n",
    "            indices = np.random.choice(self.size, batch_size, replace=False)\n",
    "            \n",
    "        return (\n",
    "            np.array([self.states[i] for i in indices]),\n",
    "            np.array([self.actions[i] for i in indices]),\n",
    "            np.array([self.rewards[i] for i in indices]),\n",
    "            np.array([self.next_states[i] for i in indices])\n",
    "        )\n",
    "        \n",
    "    def clear(self):\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.next_states = []\n",
    "        self.size = 0\n",
    "        \n",
    "    def get_all(self):\n",
    "        \"\"\"Lấy tất cả dữ liệu dưới dạng list tuples\"\"\"\n",
    "        return [(self.states[i], self.actions[i], self.rewards[i], self.next_states[i]) \n",
    "                for i in range(self.size)]\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, min_delta=0.001):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.best_value = None\n",
    "        self.counter = 0\n",
    "        self.should_stop = False\n",
    "        \n",
    "    def __call__(self, metric_value):\n",
    "        if self.best_value is None:\n",
    "            self.best_value = metric_value\n",
    "            return False\n",
    "            \n",
    "        if metric_value > self.best_value + self.min_delta:\n",
    "            self.best_value = metric_value\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            \n",
    "        if self.counter >= self.patience:\n",
    "            self.should_stop = True\n",
    "            \n",
    "        return self.should_stop\n",
    "\n",
    "class LearningRateScheduler:\n",
    "    def __init__(self, initial_lr=0.001, decay_steps=100, decay_rate=0.9, \n",
    "               warmup_steps=0, min_lr=1e-6, strategy='exponential'):\n",
    "        self.initial_lr = initial_lr\n",
    "        self.decay_steps = decay_steps\n",
    "        self.decay_rate = decay_rate\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.min_lr = min_lr\n",
    "        self.strategy = strategy\n",
    "        self.current_step = 0\n",
    "        \n",
    "    def get_lr(self):\n",
    "        # Warmup phase\n",
    "        if self.current_step < self.warmup_steps:\n",
    "            return self.initial_lr * (self.current_step / max(1, self.warmup_steps))\n",
    "            \n",
    "        # Decay phase\n",
    "        steps_after_warmup = self.current_step - self.warmup_steps\n",
    "        decay_factor = steps_after_warmup / self.decay_steps\n",
    "        \n",
    "        if self.strategy == 'exponential':\n",
    "            lr = self.initial_lr * (self.decay_rate ** decay_factor)\n",
    "        elif self.strategy == 'linear':\n",
    "            lr = self.initial_lr * (1.0 - decay_factor * (1.0 - self.decay_rate))\n",
    "        elif self.strategy == 'cosine':\n",
    "            decay = 0.5 * (1 + np.cos(np.pi * decay_factor))\n",
    "            lr = self.initial_lr * decay\n",
    "        else:\n",
    "            lr = self.initial_lr\n",
    "            \n",
    "        return max(self.min_lr, lr)\n",
    "        \n",
    "    def step(self):\n",
    "        self.current_step += 1\n",
    "        return self.get_lr()\n",
    "\n",
    "# Kiểm tra Memory Buffer\n",
    "try:\n",
    "    memory = MemoryBuffer(capacity=100)\n",
    "    \n",
    "    # Thêm một số mẫu\n",
    "    for i in range(10):\n",
    "        state = np.random.rand(20, 13)  # (time_steps, features)\n",
    "        action = np.random.randint(0, 2)\n",
    "        reward = np.random.rand()\n",
    "        next_state = np.random.rand(20, 13)\n",
    "        memory.add(state, action, reward, next_state)\n",
    "    \n",
    "    # Lấy mẫu\n",
    "    states, actions, rewards, next_states = memory.sample(5)\n",
    "    print(f\"Sampled batch shapes - states: {states.shape}, actions: {actions.shape}\")\n",
    "    \n",
    "    # Kiểm tra Early Stopping\n",
    "    early_stopping = EarlyStopping(patience=3, min_delta=0.01)\n",
    "    for i in range(5):\n",
    "        value = 0.5 + 0.1 * i if i != 2 else 0.6  # Giá trị tăng trừ bước 2\n",
    "        should_stop = early_stopping(value)\n",
    "        print(f\"Iter {i}: value={value:.2f}, stop={should_stop}\")\n",
    "    \n",
    "    # Kiểm tra LR Scheduler\n",
    "    lr_scheduler = LearningRateScheduler(\n",
    "        initial_lr=0.001, decay_steps=5, decay_rate=0.9, strategy='exponential'\n",
    "    )\n",
    "    for i in range(10):\n",
    "        lr = lr_scheduler.step()\n",
    "        print(f\"Step {i+1}: LR = {lr:.6f}\")\n",
    "    \n",
    "    print(\"Utilities tested successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Utilities test failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4319d70b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "5f4c22ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-21 13:31:31,103 - INFO - Agent 0 initialized\n",
      "2025-05-21 13:31:31,117 - INFO - States shape: (10, 20, 13), actions shape: (10,)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected action: 0, probability: 0.5678\n",
      "Training metrics: {'actor_loss': -0.6556099951267242, 'critic_loss': 0.1594378836452961, 'entropy': 0.6786390841007233, 'kl': 0.0005597588024102151}\n",
      "PPO Agent tested successfully!\n"
     ]
    }
   ],
   "source": [
    "class PPOAgent:\n",
    "    def __init__(self, config, agent_id: int):\n",
    "        self.config = config\n",
    "        self.agent_id = agent_id\n",
    "        \n",
    "        # PPO parameters\n",
    "        self.clip_ratio = config.PPO_CLIP_RATIO\n",
    "        self.target_kl = config.PPO_TARGET_KL\n",
    "        \n",
    "        # Initialize networks\n",
    "        self.actor = ActorNetwork(config.STATE_DIM, config.ACTION_DIM, config.HIDDEN_UNITS)\n",
    "        self.critic = CriticNetwork(config.STATE_DIM, config.HIDDEN_UNITS)\n",
    "        self.old_actor = ActorNetwork(config.STATE_DIM, config.ACTION_DIM, config.HIDDEN_UNITS)\n",
    "        \n",
    "        # Khởi tạo các mạng với shape đầu vào cụ thể\n",
    "        input_shape = (config.WINDOW_SIZE, config.STATE_DIM)\n",
    "        dummy_input = tf.random.normal((1,) + input_shape)\n",
    "        self.actor(dummy_input)\n",
    "        self.critic(dummy_input)\n",
    "        self.old_actor(dummy_input)\n",
    "        \n",
    "        # Initialize optimizers\n",
    "        self.actor_optimizer = tf.keras.optimizers.Adam(config.LEARNING_RATE)\n",
    "        self.critic_optimizer = tf.keras.optimizers.Adam(config.LEARNING_RATE)\n",
    "        \n",
    "        # Experience buffer - sử dụng MemoryBuffer\n",
    "        self.memory = MemoryBuffer(capacity=10000)\n",
    "        \n",
    "        # Copy weights to old actor\n",
    "        self._update_old_actor()\n",
    "        \n",
    "        logger.info(f\"Agent {agent_id} initialized\")\n",
    "        \n",
    "    def _update_old_actor(self):\n",
    "        for old_var, var in zip(self.old_actor.trainable_variables, self.actor.trainable_variables):\n",
    "            old_var.assign(var)\n",
    "    \n",
    "    def select_action(self, state: np.ndarray) -> Tuple[int, float]:\n",
    "        try:\n",
    "            # Đảm bảo state có đúng kích thước cho mô hình LSTM (batch, time_steps, features)\n",
    "            if len(state.shape) == 2:  # (time_steps, features)\n",
    "                state_tensor = tf.convert_to_tensor(state[None, :, :], dtype=tf.float32)\n",
    "            elif len(state.shape) == 1:  # (features,)\n",
    "                # Thêm cả time_steps và batch dimensions\n",
    "                state_tensor = tf.convert_to_tensor(state[None, None, :], dtype=tf.float32)\n",
    "            else:\n",
    "                state_tensor = tf.convert_to_tensor(state, dtype=tf.float32)\n",
    "            \n",
    "            action_probs = self.actor(state_tensor).numpy()[0]\n",
    "            \n",
    "            # Đảm bảo action_probs có kích thước phù hợp với ACTION_DIM\n",
    "            if len(action_probs) != self.config.ACTION_DIM:\n",
    "                logger.warning(f\"Action probs shape {len(action_probs)} ≠ ACTION_DIM {self.config.ACTION_DIM}\")\n",
    "                action_probs = np.ones(self.config.ACTION_DIM) / self.config.ACTION_DIM\n",
    "                \n",
    "            # Epsilon-greedy exploration\n",
    "            if np.random.random() < 0.1:\n",
    "                action = np.random.choice(self.config.ACTION_DIM)\n",
    "            else:\n",
    "                action = np.argmax(action_probs)\n",
    "                \n",
    "            return action, action_probs[action]\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in select_action: {e}, state shape: {state.shape}\")\n",
    "            # Trả về hành động mặc định an toàn\n",
    "            return 0, 1.0\n",
    "    \n",
    "    def train_ppo_epoch(self, memory, num_epochs=4, batch_size=64):\n",
    "        if len(memory) < batch_size:\n",
    "            return {'actor_loss': 0, 'critic_loss': 0, 'entropy': 0, 'kl': 0}\n",
    "        \n",
    "        try:\n",
    "            # Đảm bảo dữ liệu giữ nguyên định dạng 3D\n",
    "            states = np.array([exp[0] for exp in memory])  # Shape: (batch, time_steps, features)\n",
    "            actions = np.array([exp[1] for exp in memory])\n",
    "            rewards = np.array([exp[2] for exp in memory])\n",
    "            next_states = np.array([exp[3] for exp in memory])\n",
    "            \n",
    "            # Log kích thước để debug\n",
    "            logger.info(f\"States shape: {states.shape}, actions shape: {actions.shape}\")\n",
    "            \n",
    "            # Đảm bảo dữ liệu states có định dạng 3D\n",
    "            if len(states.shape) == 2:  # Nếu đã bị flatten\n",
    "                # Thử khôi phục định dạng 3D\n",
    "                states = states.reshape(states.shape[0], self.config.WINDOW_SIZE, -1)\n",
    "                next_states = next_states.reshape(next_states.shape[0], self.config.WINDOW_SIZE, -1)\n",
    "                logger.info(f\"Reshaped states to: {states.shape}\")\n",
    "            \n",
    "            # Convert to tensors\n",
    "            states_tensor = tf.convert_to_tensor(states, dtype=tf.float32)\n",
    "            next_states_tensor = tf.convert_to_tensor(next_states, dtype=tf.float32)\n",
    "            \n",
    "            # Calculate values (ensure we get a scalar per state)\n",
    "            values = self.critic(states_tensor).numpy().flatten()\n",
    "            next_values = self.critic(next_states_tensor).numpy().flatten()\n",
    "            \n",
    "            # Calculate returns and advantages\n",
    "            advantages = rewards + self.config.GAMMA * next_values - values\n",
    "            returns = rewards + self.config.GAMMA * next_values\n",
    "            \n",
    "            # Get old action probabilities\n",
    "            old_action_probs = tf.reduce_sum(\n",
    "                self.actor(states_tensor) * tf.one_hot(actions, self.config.ACTION_DIM),\n",
    "                axis=1\n",
    "            ).numpy()\n",
    "            \n",
    "            metrics = defaultdict(list)\n",
    "            \n",
    "            # Train for multiple epochs\n",
    "            for _ in range(num_epochs):\n",
    "                indices = np.random.permutation(len(states))\n",
    "                \n",
    "                for start_idx in range(0, len(states), batch_size):\n",
    "                    idx = indices[start_idx:start_idx + batch_size]\n",
    "                    if len(idx) < 3:  # Quá ít mẫu, bỏ qua\n",
    "                        continue\n",
    "                    \n",
    "                    batch_metrics = self.update_ppo(\n",
    "                        tf.convert_to_tensor(states[idx], dtype=tf.float32),\n",
    "                        tf.convert_to_tensor(actions[idx], dtype=tf.int32),\n",
    "                        tf.convert_to_tensor(advantages[idx], dtype=tf.float32),\n",
    "                        tf.convert_to_tensor(returns[idx], dtype=tf.float32),\n",
    "                        tf.convert_to_tensor(old_action_probs[idx], dtype=tf.float32)\n",
    "                    )\n",
    "                    \n",
    "                    for k, v in batch_metrics.items():\n",
    "                        metrics[k].append(v)\n",
    "                    \n",
    "                    # Early stopping if KL too large\n",
    "                    if batch_metrics['kl'] > 1.5 * self.target_kl:\n",
    "                        break\n",
    "            \n",
    "            # Update old actor\n",
    "            self._update_old_actor()\n",
    "            \n",
    "            return {k: np.mean(v) for k, v in metrics.items()}\n",
    "        \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in train_ppo_epoch: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return {'actor_loss': 0, 'critic_loss': 0, 'entropy': 0, 'kl': 0}\n",
    "    \n",
    "    # Update PPO không dùng @tf.function để dễ debug\n",
    "    def update_ppo(self, states, actions, advantages, returns, old_action_probs):\n",
    "        with tf.GradientTape() as actor_tape, tf.GradientTape() as critic_tape:\n",
    "            # Critic loss\n",
    "            values = self.critic(states)\n",
    "            values_flat = tf.reshape(values, [-1])  # Flatten để match shape với returns\n",
    "            critic_loss = tf.reduce_mean(tf.square(returns - values_flat)) * 0.5\n",
    "            \n",
    "            # Actor loss (PPO style)\n",
    "            action_probs = self.actor(states)\n",
    "            action_logprobs = tf.math.log(tf.reduce_sum(\n",
    "                action_probs * tf.one_hot(actions, self.config.ACTION_DIM),\n",
    "                axis=1\n",
    "            ) + 1e-10)\n",
    "            old_action_logprobs = tf.math.log(old_action_probs + 1e-10)\n",
    "            \n",
    "            # Calculate ratios\n",
    "            ratio = tf.exp(action_logprobs - old_action_logprobs)\n",
    "            \n",
    "            # PPO clipped objective\n",
    "            clip_adv = tf.clip_by_value(ratio, 1.0 - self.clip_ratio, 1.0 + self.clip_ratio) * advantages\n",
    "            ppo_loss = -tf.reduce_mean(tf.minimum(ratio * advantages, clip_adv))\n",
    "            \n",
    "            # Entropy bonus\n",
    "            entropy = -tf.reduce_mean(tf.reduce_sum(action_probs * tf.math.log(action_probs + 1e-10), axis=1))\n",
    "            actor_loss = ppo_loss - 0.01 * entropy\n",
    "            \n",
    "        # Compute and apply gradients\n",
    "        actor_grads = actor_tape.gradient(actor_loss, self.actor.trainable_variables)\n",
    "        critic_grads = critic_tape.gradient(critic_loss, self.critic.trainable_variables)\n",
    "        \n",
    "        self.actor_optimizer.apply_gradients(zip(actor_grads, self.actor.trainable_variables))\n",
    "        self.critic_optimizer.apply_gradients(zip(critic_grads, self.critic.trainable_variables))\n",
    "        \n",
    "        # Calculate KL for early stopping\n",
    "        old_action_probs_tensor = self.old_actor(states)\n",
    "        kl = tf.reduce_mean(tf.reduce_sum(\n",
    "            old_action_probs_tensor * tf.math.log((old_action_probs_tensor + 1e-10) / (action_probs + 1e-10)), \n",
    "            axis=1\n",
    "        ))\n",
    "        \n",
    "        # Convert to Python values for non-graph mode\n",
    "        return {\n",
    "            'actor_loss': float(actor_loss.numpy()), \n",
    "            'critic_loss': float(critic_loss.numpy()), \n",
    "            'entropy': float(entropy.numpy()), \n",
    "            'kl': float(kl.numpy())\n",
    "        }\n",
    "    \n",
    "    def store_experience(self, state, action, reward, next_state):\n",
    "        self.memory.add(state, action, reward, next_state)\n",
    "\n",
    "# Kiểm tra PPOAgent\n",
    "try:\n",
    "    # Khởi tạo agent\n",
    "    ppo_agent = PPOAgent(config, agent_id=0)\n",
    "    \n",
    "    # Tạo dữ liệu giả để thử nghiệm\n",
    "    dummy_state = np.random.rand(config.WINDOW_SIZE, config.STATE_DIM)\n",
    "    \n",
    "    # Thử nghiệm chọn hành động\n",
    "    action, prob = ppo_agent.select_action(dummy_state)\n",
    "    print(f\"Selected action: {action}, probability: {prob:.4f}\")\n",
    "    \n",
    "    # Thử nghiệm lưu trữ kinh nghiệm\n",
    "    for i in range(10):\n",
    "        state = np.random.rand(config.WINDOW_SIZE, config.STATE_DIM) \n",
    "        action = np.random.randint(0, 2)\n",
    "        reward = np.random.rand()\n",
    "        next_state = np.random.rand(config.WINDOW_SIZE, config.STATE_DIM)\n",
    "        ppo_agent.store_experience(state, action, reward, next_state)\n",
    "    \n",
    "    # Thử nghiệm train PPO\n",
    "    memory_data = ppo_agent.memory.get_all()\n",
    "    metrics = ppo_agent.train_ppo_epoch(memory_data, num_epochs=2, batch_size=4)\n",
    "    print(f\"Training metrics: {metrics}\")\n",
    "    \n",
    "    print(\"PPO Agent tested successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"PPO Agent test failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f7b3a7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "6b97f47f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-21 13:31:36,339 - INFO - Agent 0 initialized\n",
      "2025-05-21 13:31:36,502 - INFO - Agent 1 initialized\n",
      "2025-05-21 13:31:36,502 - INFO - Fog Node 0 initialized with 2 agents\n",
      "2025-05-21 13:31:36,639 - INFO - Training local with data shape: (100, 20, 13), labels shape: (100,)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fog node has 2 agents\n",
      "Processed batch of 5 samples with predictions: [1, 0, 0, 0, 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-21 13:31:38,259 - INFO - Agent 0 training with 100 experiences\n",
      "2025-05-21 13:31:38,260 - INFO - States shape: (100, 20, 13), actions shape: (100,)\n",
      "2025-05-21 13:31:39,978 - INFO - Agent 1 training with 100 experiences\n",
      "2025-05-21 13:31:39,979 - INFO - States shape: (100, 20, 13), actions shape: (100,)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local training metrics: {'actor_loss': -0.08138644276186824, 'critic_loss': 0.5040508471429348, 'entropy': 0.6749391779303551, 'kl': 0.016190933296456933}\n",
      "Fog Node tested successfully!\n"
     ]
    }
   ],
   "source": [
    "class FogNode:\n",
    "    def __init__(self, node_id: int, config):\n",
    "        self.node_id = node_id\n",
    "        self.config = config\n",
    "        self.agents = []\n",
    "        \n",
    "        # Initialize agents\n",
    "        self._initialize_agents()\n",
    "        \n",
    "        # Performance metrics\n",
    "        self.metrics = {\n",
    "            'detection_latency': [],\n",
    "            'processing_time': [],\n",
    "            'resource_usage': {'cpu': [], 'memory': []}\n",
    "        }\n",
    "        \n",
    "        logger.info(f\"Fog Node {node_id} initialized with {len(self.agents)} agents\")\n",
    "        \n",
    "    def _initialize_agents(self):\n",
    "        for i in range(self.config.NUM_AGENTS_PER_NODE):\n",
    "            agent_id = self.node_id * self.config.NUM_AGENTS_PER_NODE + i\n",
    "            agent = PPOAgent(self.config, agent_id)\n",
    "            self.agents.append(agent)\n",
    "            \n",
    "    def process_data(self, data_batch: np.ndarray) -> List[int]:\n",
    "        start_time = datetime.now()\n",
    "        \n",
    "        predictions = []\n",
    "        for sample in data_batch:\n",
    "            # Get predictions from all agents\n",
    "            agent_predictions = []\n",
    "            for agent in self.agents:\n",
    "                action, _ = agent.select_action(sample)\n",
    "                agent_predictions.append(action)\n",
    "                \n",
    "            # Majority voting\n",
    "            final_prediction = np.bincount(agent_predictions).argmax()\n",
    "            predictions.append(final_prediction)\n",
    "            \n",
    "        # Update metrics\n",
    "        processing_time = (datetime.now() - start_time).total_seconds()\n",
    "        self.metrics['processing_time'].append(processing_time)\n",
    "        self.metrics['detection_latency'].append(processing_time)\n",
    "        \n",
    "        return predictions\n",
    "        \n",
    "    def train_local(self, data: np.ndarray, labels: np.ndarray) -> Dict:\n",
    "        \"\"\"Huấn luyện agents cục bộ với PPO\"\"\"\n",
    "        try:\n",
    "            metrics = defaultdict(list)\n",
    "            \n",
    "            # Kiểm tra kích thước dữ liệu\n",
    "            logger.info(f\"Training local with data shape: {data.shape}, labels shape: {labels.shape}\")\n",
    "            \n",
    "            if len(data.shape) != 3:\n",
    "                logger.error(f\"Expected 3D data (samples, time_steps, features), got {data.shape}\")\n",
    "                return {\"error\": 1.0}\n",
    "            \n",
    "            for agent in self.agents:\n",
    "                experiences = []\n",
    "                \n",
    "                # Giới hạn số lượng mẫu để tránh OOM\n",
    "                max_samples = min(self.config.MAX_SAMPLES_PER_NODE, len(data))\n",
    "                sample_indices = np.random.choice(len(data), max_samples, replace=False)\n",
    "                \n",
    "                # Thu thập kinh nghiệm\n",
    "                for i in sample_indices:\n",
    "                    try:\n",
    "                        state = data[i]\n",
    "                        label = labels[i]\n",
    "                        \n",
    "                        action, _ = agent.select_action(state)\n",
    "                        reward = 1.0 if action == label else -1.0\n",
    "                        experiences.append((state, action, reward, state))\n",
    "                    except Exception as e:\n",
    "                        logger.error(f\"Error collecting experience: {e}\")\n",
    "                \n",
    "                # Huấn luyện với PPO\n",
    "                if experiences:\n",
    "                    try:\n",
    "                        logger.info(f\"Agent {agent.agent_id} training with {len(experiences)} experiences\")\n",
    "                        ppo_metrics = agent.train_ppo_epoch(\n",
    "                            experiences, \n",
    "                            num_epochs=self.config.PPO_EPOCHS,\n",
    "                            batch_size=min(self.config.PPO_BATCH_SIZE, len(experiences))\n",
    "                        )\n",
    "                        \n",
    "                        for k, v in ppo_metrics.items():\n",
    "                            metrics[k].append(v)\n",
    "                    except Exception as e:\n",
    "                        logger.error(f\"Error in PPO training: {e}\")\n",
    "            \n",
    "            return {k: np.mean(v) for k, v in metrics.items() if v}\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in train_local: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return {\"error\": 1.0}\n",
    "\n",
    "# Kiểm tra Fog Node\n",
    "try:\n",
    "    # Khởi tạo fog node\n",
    "    fog_node = FogNode(node_id=0, config=config)\n",
    "    print(f\"Fog node has {len(fog_node.agents)} agents\")\n",
    "    \n",
    "    # Tạo dữ liệu giả\n",
    "    batch_size, time_steps, features = 5, config.WINDOW_SIZE, config.STATE_DIM\n",
    "    dummy_data = np.random.rand(batch_size, time_steps, features)\n",
    "    \n",
    "    # Thử nghiệm xử lý dữ liệu\n",
    "    predictions = fog_node.process_data(dummy_data)\n",
    "    print(f\"Processed batch of {len(dummy_data)} samples with predictions: {predictions}\")\n",
    "    \n",
    "    # Tạo dữ liệu giả lớn hơn cho huấn luyện\n",
    "    data = np.random.rand(100, time_steps, features)\n",
    "    labels = np.random.randint(0, 2, size=100)\n",
    "    \n",
    "    # Thử nghiệm huấn luyện cục bộ\n",
    "    metrics = fog_node.train_local(data, labels)\n",
    "    print(f\"Local training metrics: {metrics}\")\n",
    "    \n",
    "    print(\"Fog Node tested successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Fog Node test failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e217e2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "5f2c7ebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-21 13:31:42,428 - INFO - Agent 0 initialized\n",
      "2025-05-21 13:31:42,588 - INFO - Agent 1 initialized\n",
      "2025-05-21 13:31:42,588 - INFO - Fog Node 0 initialized with 2 agents\n",
      "2025-05-21 13:31:42,754 - INFO - Agent 2 initialized\n",
      "2025-05-21 13:31:42,921 - INFO - Agent 3 initialized\n",
      "2025-05-21 13:31:42,922 - INFO - Fog Node 1 initialized with 2 agents\n",
      "2025-05-21 13:31:42,922 - INFO - Initialized 2 fog nodes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Federated learning system initialized with 2 fog nodes\n",
      "Data distributed to 2 nodes\n",
      "Node 0: X shape (50, 20, 13), y shape (50,)\n",
      "Node 1: X shape (50, 20, 13), y shape (50,)\n",
      "Federated Learning tested successfully!\n"
     ]
    }
   ],
   "source": [
    "class FederatedLearning:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.fog_nodes = []\n",
    "        self.global_weights = None\n",
    "        self.metrics_history = defaultdict(list)\n",
    "        \n",
    "        # Initialize fog nodes\n",
    "        self._initialize_fog_nodes()\n",
    "        \n",
    "    def _initialize_fog_nodes(self):\n",
    "        for i in range(self.config.NUM_FOG_NODES):\n",
    "            fog_node = FogNode(i, self.config)\n",
    "            self.fog_nodes.append(fog_node)\n",
    "        logger.info(f\"Initialized {len(self.fog_nodes)} fog nodes\")\n",
    "            \n",
    "    def distribute_data(self, X: np.ndarray, y: np.ndarray, \n",
    "                       distribution_type: str = None, \n",
    "                       alpha: float = None) -> List[Tuple[np.ndarray, np.ndarray]]:\n",
    "        try:\n",
    "            n_nodes = self.config.NUM_FOG_NODES\n",
    "            n_samples = len(X)\n",
    "            distributed_data = []\n",
    "            \n",
    "            # Default to config values if not specified\n",
    "            if distribution_type is None:\n",
    "                distribution_type = self.config.DATA_DISTRIBUTION_TYPE\n",
    "            if alpha is None:\n",
    "                alpha = self.config.DIRICHLET_ALPHA\n",
    "            \n",
    "            if distribution_type == 'equal':\n",
    "                # Equal distribution\n",
    "                data_per_node = n_samples // n_nodes\n",
    "                for i in range(n_nodes):\n",
    "                    start_idx = i * data_per_node\n",
    "                    end_idx = start_idx + data_per_node if i < n_nodes - 1 else n_samples\n",
    "                    distributed_data.append((X[start_idx:end_idx], y[start_idx:end_idx]))\n",
    "                    \n",
    "            elif distribution_type == 'dirichlet':\n",
    "                # Dirichlet distribution\n",
    "                proportions = np.random.dirichlet(np.repeat(alpha, n_nodes))\n",
    "                samples_per_node = np.round(proportions * n_samples).astype(int)\n",
    "                samples_per_node[-1] = n_samples - np.sum(samples_per_node[:-1])\n",
    "                \n",
    "                indices = np.random.permutation(n_samples)\n",
    "                start_idx = 0\n",
    "                \n",
    "                for n_samples_node in samples_per_node:\n",
    "                    node_indices = indices[start_idx:start_idx + n_samples_node]\n",
    "                    distributed_data.append((X[node_indices], y[node_indices]))\n",
    "                    start_idx += n_samples_node\n",
    "                \n",
    "                logger.info(f\"Dirichlet distribution (α={alpha}): {samples_per_node}\")\n",
    "                \n",
    "            elif distribution_type == 'label_based':\n",
    "                # Label-based distribution\n",
    "                unique_labels = np.unique(y)\n",
    "                n_labels = len(unique_labels)\n",
    "                labels_per_node = max(1, n_labels // n_nodes)\n",
    "                \n",
    "                for i in range(n_nodes):\n",
    "                    start_label_idx = (i * labels_per_node) % n_labels\n",
    "                    node_labels = unique_labels[start_label_idx:start_label_idx + labels_per_node]\n",
    "                    \n",
    "                    node_indices = np.isin(y, node_labels)\n",
    "                    node_X, node_y = X[node_indices], y[node_indices]\n",
    "                    \n",
    "                    # Ensure minimum data per node\n",
    "                    if len(node_X) < n_samples // (n_nodes * 2):\n",
    "                        additional_indices = np.random.choice(\n",
    "                            np.where(~node_indices)[0],\n",
    "                            size=min(n_samples // (n_nodes * 2), np.sum(~node_indices)),\n",
    "                            replace=False\n",
    "                        )\n",
    "                        node_X = np.vstack([node_X, X[additional_indices]])\n",
    "                        node_y = np.append(node_y, y[additional_indices])\n",
    "                    \n",
    "                    distributed_data.append((node_X, node_y))\n",
    "                    \n",
    "            elif distribution_type == 'weighted':\n",
    "                # Weighted distribution\n",
    "                weights = np.array([2**(i/(n_nodes-1)) for i in range(n_nodes)])\n",
    "                weights /= np.sum(weights)\n",
    "                \n",
    "                samples_per_node = np.round(weights * n_samples).astype(int)\n",
    "                samples_per_node[-1] = n_samples - np.sum(samples_per_node[:-1])\n",
    "                \n",
    "                indices = np.random.permutation(n_samples)\n",
    "                start_idx = 0\n",
    "                \n",
    "                for n_samples_node in samples_per_node:\n",
    "                    node_indices = indices[start_idx:start_idx + n_samples_node]\n",
    "                    distributed_data.append((X[node_indices], y[node_indices]))\n",
    "                    start_idx += n_samples_node\n",
    "                \n",
    "                logger.info(f\"Weighted distribution: {samples_per_node}\")\n",
    "                \n",
    "            else:\n",
    "                raise ValueError(f\"Unknown distribution type: {distribution_type}\")\n",
    "                \n",
    "            return distributed_data\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error distributing data: {str(e)}\")\n",
    "            raise\n",
    "            \n",
    "    def train_round(self, distributed_data: List[Tuple[np.ndarray, np.ndarray]]) -> Dict:\n",
    "        try:\n",
    "            round_metrics = defaultdict(list)\n",
    "            \n",
    "            # Local training on each fog node\n",
    "            for fog_node, (node_X, node_y) in zip(self.fog_nodes, distributed_data):\n",
    "                local_metrics = fog_node.train_local(node_X, node_y)\n",
    "                \n",
    "                for k, v in local_metrics.items():\n",
    "                    round_metrics[k].append(v)\n",
    "                    \n",
    "            # Aggregate models\n",
    "            self._aggregate_models()\n",
    "            \n",
    "            # Update fog nodes with global model\n",
    "            self._update_fog_nodes()\n",
    "            \n",
    "            # Calculate average metrics\n",
    "            avg_metrics = {k: np.mean(v) for k, v in round_metrics.items()}\n",
    "            \n",
    "            # Store metrics history\n",
    "            for k, v in avg_metrics.items():\n",
    "                self.metrics_history[k].append(v)\n",
    "                \n",
    "            return avg_metrics\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in training round: {str(e)}\")\n",
    "            raise\n",
    "            \n",
    "    def _aggregate_models(self):\n",
    "        try:\n",
    "            if not self.fog_nodes:\n",
    "                raise ValueError(\"No fog nodes available\")\n",
    "                \n",
    "            aggregated_weights = {}\n",
    "            \n",
    "            # Get first agent's structure\n",
    "            first_agent = self.fog_nodes[0].agents[0]\n",
    "            \n",
    "            # Initialize aggregated weights with zeros\n",
    "            for var in first_agent.actor.trainable_variables:\n",
    "                aggregated_weights[f'actor_{var.name}'] = tf.zeros_like(var)\n",
    "            for var in first_agent.critic.trainable_variables:\n",
    "                aggregated_weights[f'critic_{var.name}'] = tf.zeros_like(var)\n",
    "                \n",
    "            # Sum weights from all agents\n",
    "            num_agents = self.config.NUM_FOG_NODES * self.config.NUM_AGENTS_PER_NODE\n",
    "            for fog_node in self.fog_nodes:\n",
    "                for agent in fog_node.agents:\n",
    "                    for var in agent.actor.trainable_variables:\n",
    "                        aggregated_weights[f'actor_{var.name}'] += var / num_agents\n",
    "                    for var in agent.critic.trainable_variables:\n",
    "                        aggregated_weights[f'critic_{var.name}'] += var / num_agents\n",
    "                        \n",
    "            self.global_weights = aggregated_weights\n",
    "            logger.info(\"Models aggregated successfully\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error aggregating models: {str(e)}\")\n",
    "            raise\n",
    "            \n",
    "    def _update_fog_nodes(self):\n",
    "        try:\n",
    "            if self.global_weights is None:\n",
    "                raise ValueError(\"Global weights not available\")\n",
    "                \n",
    "            for fog_node in self.fog_nodes:\n",
    "                for agent in fog_node.agents:\n",
    "                    # Update actor\n",
    "                    for var in agent.actor.trainable_variables:\n",
    "                        var.assign(self.global_weights[f'actor_{var.name}'])\n",
    "                    # Update critic\n",
    "                    for var in agent.critic.trainable_variables:\n",
    "                        var.assign(self.global_weights[f'critic_{var.name}'])\n",
    "                    # Update old actor (for PPO)\n",
    "                    agent._update_old_actor()\n",
    "                        \n",
    "            logger.info(\"Fog nodes updated with global weights\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error updating fog nodes: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "# Kiểm tra Federated Learning\n",
    "try:\n",
    "    # Khởi tạo federated learning\n",
    "    fl_system = FederatedLearning(config)\n",
    "    print(f\"Federated learning system initialized with {len(fl_system.fog_nodes)} fog nodes\")\n",
    "    \n",
    "    # Tạo dữ liệu giả\n",
    "    batch_size, time_steps, features = 100, config.WINDOW_SIZE, config.STATE_DIM\n",
    "    X = np.random.rand(batch_size, time_steps, features)\n",
    "    y = np.random.randint(0, 2, size=batch_size)\n",
    "    \n",
    "    # Phân phối dữ liệu\n",
    "    distributed_data = fl_system.distribute_data(X, y)\n",
    "    print(f\"Data distributed to {len(distributed_data)} nodes\")\n",
    "    for i, (node_X, node_y) in enumerate(distributed_data):\n",
    "        print(f\"Node {i}: X shape {node_X.shape}, y shape {node_y.shape}\")\n",
    "    \n",
    "    # Kiểm tra huấn luyện một vòng nếu cần thiết\n",
    "    # Vì huấn luyện khá nặng nên chỉ bình luận ở đây\n",
    "    # metrics = fl_system.train_round(distributed_data)\n",
    "    # print(f\"Training round metrics: {metrics}\")\n",
    "    \n",
    "    print(\"Federated Learning tested successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Federated Learning test failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b9b590",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "50396f2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-21 13:31:46,431 - INFO - Loading dataset...\n",
      "2025-05-21 13:31:47,331 - INFO - Dataset loaded: 225745 samples, 85 features\n",
      "2025-05-21 13:31:47,361 - INFO - Sampled dataset: 10000 samples\n",
      "2025-05-21 13:31:47,361 - INFO - Handling missing values...\n",
      "2025-05-21 13:31:47,369 - INFO - Normalizing features...\n",
      "2025-05-21 13:31:47,375 - INFO - Processing labels...\n",
      "2025-05-21 13:31:47,377 - INFO - Label mapping: {'BENIGN': 0, 'DDoS': 1}\n",
      "2025-05-21 13:31:47,378 - INFO - Creating time windows...\n",
      "2025-05-21 13:31:47,385 - INFO - Windows shape: (9981, 20, 13), labels shape: (9981,)\n",
      "2025-05-21 13:31:47,386 - INFO - Splitting data...\n",
      "2025-05-21 13:31:47,395 - INFO - Data preprocessing completed: X_train shape (7984, 20, 13), y_train shape (7984,)\n",
      "2025-05-21 13:31:47,396 - INFO - Class distribution in training set: [3388 4596]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data preprocessed: X_train shape (7984, 20, 13), y_train shape (7984,)\n"
     ]
    }
   ],
   "source": [
    "class DataPreprocessor:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.scaler = MinMaxScaler()\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        \n",
    "    def preprocess_data(self) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "        try:\n",
    "            # Load data\n",
    "            logger.info(\"Loading dataset...\")\n",
    "            df = pd.read_csv(self.config.DATA_PATH)\n",
    "            \n",
    "            # Log dataset info\n",
    "            logger.info(f\"Dataset loaded: {df.shape[0]} samples, {df.shape[1]} features\")\n",
    "            \n",
    "            # Sampling data nếu cần\n",
    "            if self.config.USE_DATA_SAMPLING:\n",
    "                sample_size = min(self.config.MAX_SAMPLES, len(df))  # Giới hạn kích thước\n",
    "                df = df.sample(sample_size, random_state=42)\n",
    "                logger.info(f\"Sampled dataset: {df.shape[0]} samples\")\n",
    "            \n",
    "            # Handle missing values in features\n",
    "            logger.info(\"Handling missing values...\")\n",
    "            features = df[self.config.SELECTED_FEATURES].copy()\n",
    "            features.fillna(0, inplace=True)\n",
    "            \n",
    "            # Normalize features\n",
    "            logger.info(\"Normalizing features...\")\n",
    "            normalized_features = self.scaler.fit_transform(features)\n",
    "            \n",
    "            # Process labels - Encode the label column\n",
    "            logger.info(\"Processing labels...\")\n",
    "            labels = self.label_encoder.fit_transform(df[self.config.LABEL_COLUMN])\n",
    "            \n",
    "            # Log label encoding\n",
    "            label_mapping = dict(zip(self.label_encoder.classes_, \n",
    "                                     self.label_encoder.transform(self.label_encoder.classes_)))\n",
    "            logger.info(f\"Label mapping: {label_mapping}\")\n",
    "            \n",
    "            # Create windows\n",
    "            logger.info(\"Creating time windows...\")\n",
    "            X = self._create_windows(normalized_features)\n",
    "            y = labels[self.config.WINDOW_SIZE-1:]\n",
    "            \n",
    "            # Log shape\n",
    "            logger.info(f\"Windows shape: {X.shape}, labels shape: {y.shape}\")\n",
    "            \n",
    "            # Split data\n",
    "            logger.info(\"Splitting data...\")\n",
    "            X_train, X_test, y_train, y_test = train_test_split(\n",
    "                X, y, test_size=0.2, random_state=42, stratify=y\n",
    "            )\n",
    "            \n",
    "            logger.info(f\"Data preprocessing completed: X_train shape {X_train.shape}, y_train shape {y_train.shape}\")\n",
    "            logger.info(f\"Class distribution in training set: {np.bincount(y_train)}\")\n",
    "            return X_train, X_test, y_train, y_test\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in preprocessing: {str(e)}\")\n",
    "            raise\n",
    "            \n",
    "    def _create_windows(self, data: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Create time windows from data\"\"\"\n",
    "        windows = []\n",
    "        for i in range(len(data) - self.config.WINDOW_SIZE + 1):\n",
    "            window = data[i:i + self.config.WINDOW_SIZE]\n",
    "            windows.append(window)\n",
    "        return np.array(windows)\n",
    "\n",
    "# Kiểm tra Data Preprocessor - may mắn thì chạy nếu có đủ dữ liệu\n",
    "# Nếu không, chỉ cần kiểm tra class đã được định nghĩa\n",
    "try:\n",
    "    if os.path.exists(config.DATA_PATH):\n",
    "        preprocessor = DataPreprocessor(config)\n",
    "        X_train, X_test, y_train, y_test = preprocessor.preprocess_data()\n",
    "        print(f\"Data preprocessed: X_train shape {X_train.shape}, y_train shape {y_train.shape}\")\n",
    "    else:\n",
    "        print(f\"Data file not found at {config.DATA_PATH}, skipping preprocessing test\")\n",
    "        print(\"DataPreprocessor class defined successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"DataPreprocessor test failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4e4c64",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "d4c3c0b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR:tensorflow:==================================\n",
      "Object was never used (type <class 'tensorflow.python.ops.tensor_array_ops.TensorArray'>):\n",
      "<tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x12f36e5e0>\n",
      "If you want to mark it as used call its \"mark_used()\" method.\n",
      "It was originally created here:\n",
      "  File \"/Users/macbook/venv/lib/python3.9/site-packages/keras/src/backend/tensorflow/rnn.py\", line 419, in <genexpr>\n",
      "    ta.write(ta_index_to_write, out)  File \"/Users/macbook/venv/lib/python3.9/site-packages/tensorflow/python/util/tf_should_use.py\", line 288, in wrapped\n",
      "    return _add_should_use_warning(fn(*args, **kwargs),\n",
      "==================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-21 13:31:50,031 - ERROR - ==================================\n",
      "Object was never used (type <class 'tensorflow.python.ops.tensor_array_ops.TensorArray'>):\n",
      "<tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x12f36e5e0>\n",
      "If you want to mark it as used call its \"mark_used()\" method.\n",
      "It was originally created here:\n",
      "  File \"/Users/macbook/venv/lib/python3.9/site-packages/keras/src/backend/tensorflow/rnn.py\", line 419, in <genexpr>\n",
      "    ta.write(ta_index_to_write, out)  File \"/Users/macbook/venv/lib/python3.9/site-packages/tensorflow/python/util/tf_should_use.py\", line 288, in wrapped\n",
      "    return _add_should_use_warning(fn(*args, **kwargs),\n",
      "==================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR:tensorflow:==================================\n",
      "Object was never used (type <class 'tensorflow.python.ops.tensor_array_ops.TensorArray'>):\n",
      "<tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x313deb7c0>\n",
      "If you want to mark it as used call its \"mark_used()\" method.\n",
      "It was originally created here:\n",
      "  File \"/Users/macbook/venv/lib/python3.9/site-packages/keras/src/backend/tensorflow/rnn.py\", line 419, in <genexpr>\n",
      "    ta.write(ta_index_to_write, out)  File \"/Users/macbook/venv/lib/python3.9/site-packages/tensorflow/python/util/tf_should_use.py\", line 288, in wrapped\n",
      "    return _add_should_use_warning(fn(*args, **kwargs),\n",
      "==================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-21 13:31:50,032 - ERROR - ==================================\n",
      "Object was never used (type <class 'tensorflow.python.ops.tensor_array_ops.TensorArray'>):\n",
      "<tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x313deb7c0>\n",
      "If you want to mark it as used call its \"mark_used()\" method.\n",
      "It was originally created here:\n",
      "  File \"/Users/macbook/venv/lib/python3.9/site-packages/keras/src/backend/tensorflow/rnn.py\", line 419, in <genexpr>\n",
      "    ta.write(ta_index_to_write, out)  File \"/Users/macbook/venv/lib/python3.9/site-packages/tensorflow/python/util/tf_should_use.py\", line 288, in wrapped\n",
      "    return _add_should_use_warning(fn(*args, **kwargs),\n",
      "==================================\n",
      "2025-05-21 13:31:50,432 - INFO - Training history plot saved to /Users/macbook/Desktop/FL-RL-Dos detection/Multi agents RL-FL-Fog/Multi agents RL-FL-Fog/results/training_history.png\n",
      "2025-05-21 13:31:50,494 - INFO - Confusion matrix plot saved to /Users/macbook/Desktop/FL-RL-Dos detection/Multi agents RL-FL-Fog/Multi agents RL-FL-Fog/results/confusion_matrix.png\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Detection Performance Analysis:\n",
      "Accuracy: 0.7500\n",
      "Precision: 0.7500\n",
      "Recall: 0.7500\n",
      "F1 Score: 0.7500\n",
      "Pipeline & Analyzer test passed!\n"
     ]
    }
   ],
   "source": [
    "class TrainingPipeline:\n",
    "    def __init__(self, config, fl_system, X_train, y_train, X_test, y_test):\n",
    "        self.config = config\n",
    "        self.fl_system = fl_system\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.X_test = X_test\n",
    "        self.y_test = y_test\n",
    "        self.metrics_history = {}\n",
    "        \n",
    "        logger.info(\"Training pipeline initialized\")\n",
    "        \n",
    "    def train(self):\n",
    "        try:\n",
    "            # Initialize early stopping and LR scheduler\n",
    "            early_stopping = EarlyStopping(\n",
    "                patience=self.config.EARLY_STOPPING_PATIENCE,\n",
    "                min_delta=self.config.EARLY_STOPPING_MIN_DELTA\n",
    "            )\n",
    "            \n",
    "            lr_scheduler = LearningRateScheduler(\n",
    "                initial_lr=self.config.LEARNING_RATE,\n",
    "                decay_steps=self.config.LR_DECAY_STEPS,\n",
    "                decay_rate=self.config.LR_DECAY_RATE,\n",
    "                warmup_steps=self.config.LR_WARMUP_STEPS,\n",
    "                min_lr=self.config.LR_MIN,\n",
    "                strategy=self.config.LR_STRATEGY\n",
    "            )\n",
    "            \n",
    "            # Distribute data with specified distribution\n",
    "            distributed_data = self.fl_system.distribute_data(\n",
    "                self.X_train, self.y_train,\n",
    "                distribution_type=self.config.DATA_DISTRIBUTION_TYPE,\n",
    "                alpha=self.config.DIRICHLET_ALPHA\n",
    "            )\n",
    "            \n",
    "            # Training rounds\n",
    "            start_time = datetime.now()\n",
    "            best_metrics = None\n",
    "            \n",
    "            for round in range(self.config.FL_ROUNDS):\n",
    "                logger.info(f\"Starting round {round + 1}/{self.config.FL_ROUNDS}\")\n",
    "                \n",
    "                # Update learning rate\n",
    "                current_lr = lr_scheduler.step()\n",
    "                logger.info(f\"Current learning rate: {current_lr:.6f}\")\n",
    "                \n",
    "                # Update learning rate for all agents\n",
    "                for fog_node in self.fl_system.fog_nodes:\n",
    "                    for agent in fog_node.agents:\n",
    "                        agent.actor_optimizer.learning_rate.assign(current_lr)\n",
    "                        agent.critic_optimizer.learning_rate.assign(current_lr)\n",
    "                \n",
    "                # Execute training round\n",
    "                round_metrics = self.fl_system.train_round(distributed_data)\n",
    "                \n",
    "                # Evaluate\n",
    "                test_metrics = self.evaluate()\n",
    "                \n",
    "                # Log metrics\n",
    "                self._log_metrics(round, round_metrics, test_metrics)\n",
    "                \n",
    "                # Save checkpoints\n",
    "                if (round + 1) % self.config.CHECKPOINT_FREQUENCY == 0:\n",
    "                    self._save_checkpoint(round)\n",
    "                    \n",
    "                # Save best model\n",
    "                if best_metrics is None or test_metrics['f1'] > best_metrics['f1']:\n",
    "                    best_metrics = test_metrics\n",
    "                    self._save_best_model()\n",
    "                    logger.info(f\"New best model saved with F1: {test_metrics['f1']:.4f}\")\n",
    "                \n",
    "                # Check for early stopping\n",
    "                if early_stopping(test_metrics['f1']):\n",
    "                    logger.info(f\"Early stopping triggered after {round + 1} rounds\")\n",
    "                    break\n",
    "                    \n",
    "            # Final evaluation\n",
    "            if early_stopping.should_stop:\n",
    "                self._load_best_model()\n",
    "                logger.info(\"Loaded best model for final evaluation\")\n",
    "                \n",
    "            total_time = (datetime.now() - start_time).total_seconds() / 60\n",
    "            logger.info(f\"Training completed in {total_time:.2f} minutes\")\n",
    "            \n",
    "            final_metrics = self.evaluate()\n",
    "            self._save_results(final_metrics)\n",
    "            \n",
    "            return final_metrics\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in training pipeline: {str(e)}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            raise\n",
    "            \n",
    "    def evaluate(self) -> Dict:\n",
    "        try:\n",
    "            predictions = []\n",
    "            \n",
    "            # Get predictions from all fog nodes\n",
    "            for i in range(0, len(self.X_test), self.config.BATCH_SIZE):\n",
    "                batch_X = self.X_test[i:i + self.config.BATCH_SIZE]\n",
    "                batch_predictions = []\n",
    "                \n",
    "                # Get predictions from each fog node\n",
    "                for fog_node in self.fl_system.fog_nodes:\n",
    "                    node_predictions = fog_node.process_data(batch_X)\n",
    "                    batch_predictions.append(node_predictions)\n",
    "                    \n",
    "                # Aggregate predictions using majority voting\n",
    "                batch_predictions = np.array(batch_predictions)\n",
    "                final_predictions = np.array([np.bincount(batch_predictions[:, j]).argmax() \n",
    "                                            for j in range(batch_predictions.shape[1])])\n",
    "                predictions.extend(final_predictions)\n",
    "                \n",
    "            # Calculate metrics\n",
    "            predictions = np.array(predictions)\n",
    "            y_true = self.y_test[:len(predictions)]\n",
    "            \n",
    "            metrics = {\n",
    "                'accuracy': accuracy_score(y_true, predictions),\n",
    "                'precision': precision_score(y_true, predictions),\n",
    "                'recall': recall_score(y_true, predictions),\n",
    "                'f1': f1_score(y_true, predictions),\n",
    "                'predictions': predictions  # Store for confusion matrix\n",
    "            }\n",
    "            \n",
    "            return metrics\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in evaluation: {str(e)}\")\n",
    "            raise\n",
    "            \n",
    "    def _log_metrics(self, round: int, train_metrics: Dict, test_metrics: Dict):\n",
    "        logger.info(f\"\\nRound {round + 1} Metrics:\")\n",
    "        logger.info(\"Training:\")\n",
    "        for k, v in train_metrics.items():\n",
    "            logger.info(f\"{k}: {v:.4f}\")\n",
    "            if 'train_' + k not in self.metrics_history:\n",
    "                self.metrics_history['train_' + k] = []\n",
    "            self.metrics_history['train_' + k].append(v)\n",
    "            \n",
    "        logger.info(\"\\nTesting:\")\n",
    "        for k, v in test_metrics.items():\n",
    "            if k != 'predictions':\n",
    "                logger.info(f\"{k}: {v:.4f}\")\n",
    "                if 'test_' + k not in self.metrics_history:\n",
    "                    self.metrics_history['test_' + k] = []\n",
    "                self.metrics_history['test_' + k].append(v)\n",
    "            \n",
    "    def _save_checkpoint(self, round: int):\n",
    "        try:\n",
    "            checkpoint_path = os.path.join(\n",
    "                self.config.CHECKPOINT_DIR,\n",
    "                f'checkpoint_round_{round + 1}'\n",
    "            )\n",
    "            os.makedirs(checkpoint_path, exist_ok=True)\n",
    "            \n",
    "            # Save global weights\n",
    "            with open(os.path.join(checkpoint_path, 'global_weights.pkl'), 'wb') as f:\n",
    "                pickle.dump(self.fl_system.global_weights, f)\n",
    "            \n",
    "            logger.info(f\"Checkpoint saved for round {round + 1}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error saving checkpoint: {str(e)}\")\n",
    "            \n",
    "    def _save_best_model(self):\n",
    "        try:\n",
    "            best_model_path = os.path.join(\n",
    "                self.config.CHECKPOINT_DIR,\n",
    "                'best_model'\n",
    "            )\n",
    "            os.makedirs(best_model_path, exist_ok=True)\n",
    "            \n",
    "            # Save global weights\n",
    "            with open(os.path.join(best_model_path, 'global_weights.pkl'), 'wb') as f:\n",
    "                pickle.dump(self.fl_system.global_weights, f)\n",
    "            \n",
    "            logger.info(\"Best model saved successfully\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error saving best model: {str(e)}\")\n",
    "            \n",
    "    def _load_best_model(self):\n",
    "        try:\n",
    "            best_model_path = os.path.join(\n",
    "                self.config.CHECKPOINT_DIR,\n",
    "                'best_model',\n",
    "                'global_weights.pkl'\n",
    "            )\n",
    "            \n",
    "            if os.path.exists(best_model_path):\n",
    "                # Load global weights\n",
    "                with open(best_model_path, 'rb') as f:\n",
    "                    self.fl_system.global_weights = pickle.load(f)\n",
    "                \n",
    "                self.fl_system._update_fog_nodes()\n",
    "                \n",
    "                logger.info(\"Best model loaded successfully\")\n",
    "            else:\n",
    "                logger.warning(f\"Best model not found at {best_model_path}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading best model: {str(e)}\")\n",
    "            \n",
    "    def _save_results(self, final_metrics: Dict):\n",
    "        try:\n",
    "            results_path = os.path.join(\n",
    "                self.config.RESULTS_DIR,\n",
    "                'final_results.json'\n",
    "            )\n",
    "            \n",
    "            # Loại bỏ predictions từ final_metrics để có thể lưu dưới dạng JSON\n",
    "            metrics_to_save = {k: float(v) for k, v in final_metrics.items() if k != 'predictions'}\n",
    "            \n",
    "            results = {\n",
    "                'timestamp': self.config.TIMESTAMP,\n",
    "                'user': self.config.USER,\n",
    "                'final_metrics': metrics_to_save,\n",
    "                'training_history': {k: [float(x) for x in v] for k, v in self.metrics_history.items()}\n",
    "            }\n",
    "            \n",
    "            with open(results_path, 'w') as f:\n",
    "                json.dump(results, f, indent=4)\n",
    "                \n",
    "            logger.info(\"Final results saved successfully\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error saving results: {str(e)}\")\n",
    "\n",
    "class ResultsAnalyzer:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        \n",
    "    def plot_training_history(self, metrics_history):\n",
    "        try:\n",
    "            plt.figure(figsize=(18, 10))\n",
    "            \n",
    "            # Plot losses\n",
    "            plt.subplot(2, 3, 1)\n",
    "            plt.plot(metrics_history.get('train_actor_loss', []), label='Actor Loss')\n",
    "            plt.plot(metrics_history.get('train_critic_loss', []), label='Critic Loss')\n",
    "            plt.title('Training Losses')\n",
    "            plt.xlabel('Round')\n",
    "            plt.ylabel('Loss')\n",
    "            plt.legend()\n",
    "            \n",
    "            # Plot accuracy\n",
    "            plt.subplot(2, 3, 2)\n",
    "            plt.plot(metrics_history.get('test_accuracy', []), label='Accuracy')\n",
    "            plt.title('Model Accuracy')\n",
    "            plt.xlabel('Round')\n",
    "            plt.ylabel('Accuracy')\n",
    "            plt.grid(True)\n",
    "            \n",
    "            # Plot F1 Score\n",
    "            plt.subplot(2, 3, 3)\n",
    "            plt.plot(metrics_history.get('test_f1', []), label='F1 Score')\n",
    "            plt.title('F1 Score')\n",
    "            plt.xlabel('Round')\n",
    "            plt.ylabel('F1 Score')\n",
    "            plt.grid(True)\n",
    "            \n",
    "            # Plot KL divergence\n",
    "            plt.subplot(2, 3, 4)\n",
    "            plt.plot(metrics_history.get('train_kl', []), label='KL Divergence')\n",
    "            plt.title('KL Divergence')\n",
    "            plt.xlabel('Round')\n",
    "            plt.ylabel('KL')\n",
    "            plt.yscale('log')\n",
    "            plt.grid(True)\n",
    "            \n",
    "            # Plot Precision/Recall\n",
    "            plt.subplot(2, 3, 5)\n",
    "            plt.plot(metrics_history.get('test_precision', []), label='Precision')\n",
    "            plt.plot(metrics_history.get('test_recall', []), label='Recall')\n",
    "            plt.title('Precision & Recall')\n",
    "            plt.xlabel('Round')\n",
    "            plt.ylabel('Score')\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "            \n",
    "            # Plot entropy\n",
    "            plt.subplot(2, 3, 6)\n",
    "            plt.plot(metrics_history.get('train_entropy', []), label='Entropy')\n",
    "            plt.title('Policy Entropy')\n",
    "            plt.xlabel('Round')\n",
    "            plt.ylabel('Entropy')\n",
    "            plt.grid(True)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            \n",
    "            output_path = os.path.join(self.config.RESULTS_DIR, 'training_history.png')\n",
    "            plt.savefig(output_path)\n",
    "            plt.close()\n",
    "            \n",
    "            logger.info(f\"Training history plot saved to {output_path}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error plotting training history: {e}\")\n",
    "        \n",
    "    def plot_confusion_matrix(self, y_true, y_pred):\n",
    "        try:\n",
    "            cm = confusion_matrix(y_true[:len(y_pred)], y_pred)\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "            plt.title('Confusion Matrix')\n",
    "            plt.ylabel('True Label')\n",
    "            plt.xlabel('Predicted Label')\n",
    "            \n",
    "            output_path = os.path.join(self.config.RESULTS_DIR, 'confusion_matrix.png')\n",
    "            plt.savefig(output_path)\n",
    "            plt.close()\n",
    "            \n",
    "            logger.info(f\"Confusion matrix plot saved to {output_path}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error plotting confusion matrix: {e}\")\n",
    "        \n",
    "    def analyze_detection_performance(self, y_true, y_pred):\n",
    "        try:\n",
    "            metrics = {\n",
    "                'Accuracy': accuracy_score(y_true[:len(y_pred)], y_pred),\n",
    "                'Precision': precision_score(y_true[:len(y_pred)], y_pred),\n",
    "                'Recall': recall_score(y_true[:len(y_pred)], y_pred),\n",
    "                'F1 Score': f1_score(y_true[:len(y_pred)], y_pred)\n",
    "            }\n",
    "            \n",
    "            print(\"\\nDetection Performance Analysis:\")\n",
    "            for metric, value in metrics.items():\n",
    "                print(f\"{metric}: {value:.4f}\")\n",
    "                \n",
    "            return metrics\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error analyzing detection performance: {e}\")\n",
    "            return {}\n",
    "\n",
    "# Kiểm tra Pipeline và Analyzer\n",
    "try:\n",
    "    # Tạo dữ liệu giả để kiểm tra\n",
    "    analyzer = ResultsAnalyzer(config)\n",
    "    \n",
    "    # Tạo metrics history giả\n",
    "    fake_metrics = {\n",
    "        'train_actor_loss': [0.5, 0.4, 0.3],\n",
    "        'train_critic_loss': [0.4, 0.3, 0.2],\n",
    "        'train_entropy': [0.1, 0.08, 0.06],\n",
    "        'train_kl': [0.02, 0.015, 0.01],\n",
    "        'test_accuracy': [0.7, 0.75, 0.8],\n",
    "        'test_precision': [0.65, 0.7, 0.75],\n",
    "        'test_recall': [0.7, 0.75, 0.8],\n",
    "        'test_f1': [0.675, 0.725, 0.775]\n",
    "    }\n",
    "    \n",
    "    # Thử plot training history\n",
    "    analyzer.plot_training_history(fake_metrics)\n",
    "    \n",
    "    # Tạo dữ liệu giả cho confusion matrix\n",
    "    y_true = np.array([0, 0, 1, 1, 0, 1, 0, 1])\n",
    "    y_pred = np.array([0, 1, 1, 1, 0, 0, 0, 1])\n",
    "    \n",
    "    # Thử plot confusion matrix\n",
    "    analyzer.plot_confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    # Thử analyze detection performance\n",
    "    results = analyzer.analyze_detection_performance(y_true, y_pred)\n",
    "    \n",
    "    print(\"Pipeline & Analyzer test passed!\")\n",
    "except Exception as e:\n",
    "    print(f\"Pipeline & Analyzer test failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a258540f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "ec217eac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-21 13:33:44,320 - INFO - Loading dataset...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting DDoS Detection with Federated Learning and PPO...\n",
      "Data path: /Users/macbook/Desktop/FL-RL-Dos detection/Multi agents RL-FL-Fog/Multi agents RL-FL-Fog/data/Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv\n",
      "Selected features: 13 features\n",
      "Label column:  Label\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-21 13:33:45,234 - INFO - Dataset loaded: 225745 samples, 85 features\n",
      "2025-05-21 13:33:45,267 - INFO - Sampled dataset: 10000 samples\n",
      "2025-05-21 13:33:45,268 - INFO - Handling missing values...\n",
      "2025-05-21 13:33:45,273 - INFO - Normalizing features...\n",
      "2025-05-21 13:33:45,278 - INFO - Processing labels...\n",
      "2025-05-21 13:33:45,281 - INFO - Label mapping: {'BENIGN': 0, 'DDoS': 1}\n",
      "2025-05-21 13:33:45,281 - INFO - Creating time windows...\n",
      "2025-05-21 13:33:45,291 - INFO - Windows shape: (9981, 20, 13), labels shape: (9981,)\n",
      "2025-05-21 13:33:45,291 - INFO - Splitting data...\n",
      "2025-05-21 13:33:45,307 - INFO - Data preprocessing completed: X_train shape (7984, 20, 13), y_train shape (7984,)\n",
      "2025-05-21 13:33:45,308 - INFO - Class distribution in training set: [3388 4596]\n",
      "2025-05-21 13:33:45,484 - INFO - Agent 0 initialized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (7984, 20, 13)\n",
      "Testing data shape: (1997, 20, 13)\n",
      "Class distribution in training: [3388 4596]\n",
      "Class distribution in testing: [ 847 1150]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-21 13:33:45,648 - INFO - Agent 1 initialized\n",
      "2025-05-21 13:33:45,648 - INFO - Fog Node 0 initialized with 2 agents\n",
      "2025-05-21 13:33:45,808 - INFO - Agent 2 initialized\n",
      "2025-05-21 13:33:45,961 - INFO - Agent 3 initialized\n",
      "2025-05-21 13:33:45,962 - INFO - Fog Node 1 initialized with 2 agents\n",
      "2025-05-21 13:33:45,962 - INFO - Initialized 2 fog nodes\n",
      "2025-05-21 13:33:45,962 - INFO - Training pipeline initialized\n",
      "2025-05-21 13:33:45,963 - INFO - Starting round 1/2\n",
      "2025-05-21 13:33:45,963 - INFO - Current learning rate: 0.001000\n",
      "2025-05-21 13:33:45,964 - INFO - Training local with data shape: (3992, 20, 13), labels shape: (3992,)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting federated training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-21 13:34:23,794 - INFO - Agent 0 training with 3000 experiences\n",
      "2025-05-21 13:34:23,807 - INFO - States shape: (3000, 20, 13), actions shape: (3000,)\n",
      "2025-05-21 13:35:01,523 - INFO - Agent 1 training with 3000 experiences\n",
      "2025-05-21 13:35:01,528 - INFO - States shape: (3000, 20, 13), actions shape: (3000,)\n",
      "2025-05-21 13:35:03,042 - INFO - Training local with data shape: (3992, 20, 13), labels shape: (3992,)\n",
      "2025-05-21 13:35:40,023 - INFO - Agent 2 training with 3000 experiences\n",
      "2025-05-21 13:35:40,031 - INFO - States shape: (3000, 20, 13), actions shape: (3000,)\n",
      "2025-05-21 13:36:17,296 - INFO - Agent 3 training with 3000 experiences\n",
      "2025-05-21 13:36:17,301 - INFO - States shape: (3000, 20, 13), actions shape: (3000,)\n",
      "2025-05-21 13:36:19,037 - ERROR - Error aggregating models: {{function_node __wrapped__AddV2_device_/job:localhost/replica:0/task:0/device:CPU:0}} Incompatible shapes: [32,2] vs. [13,256] [Op:AddV2] name: \n",
      "2025-05-21 13:36:19.037491: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: INVALID_ARGUMENT: Incompatible shapes: [32,2] vs. [13,256]\n",
      "2025-05-21 13:36:19,038 - ERROR - Error in training round: {{function_node __wrapped__AddV2_device_/job:localhost/replica:0/task:0/device:CPU:0}} Incompatible shapes: [32,2] vs. [13,256] [Op:AddV2] name: \n",
      "2025-05-21 13:36:19,038 - ERROR - Error in training pipeline: {{function_node __wrapped__AddV2_device_/job:localhost/replica:0/task:0/device:CPU:0}} Incompatible shapes: [32,2] vs. [13,256] [Op:AddV2] name: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during execution: {{function_node __wrapped__AddV2_device_/job:localhost/replica:0/task:0/device:CPU:0}} Incompatible shapes: [32,2] vs. [13,256] [Op:AddV2] name: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/v0/t5xgy88d26bctff9sd0szwc80000gn/T/ipykernel_23656/1212352700.py\", line 55, in train\n",
      "    round_metrics = self.fl_system.train_round(distributed_data)\n",
      "  File \"/var/folders/v0/t5xgy88d26bctff9sd0szwc80000gn/T/ipykernel_23656/4016177540.py\", line 119, in train_round\n",
      "    self._aggregate_models()\n",
      "  File \"/var/folders/v0/t5xgy88d26bctff9sd0szwc80000gn/T/ipykernel_23656/4016177540.py\", line 158, in _aggregate_models\n",
      "    aggregated_weights[f'actor_{var.name}'] += var / num_agents\n",
      "  File \"/Users/macbook/venv/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py\", line 153, in error_handler\n",
      "    raise e.with_traceback(filtered_tb) from None\n",
      "  File \"/Users/macbook/venv/lib/python3.9/site-packages/tensorflow/python/framework/ops.py\", line 6006, in raise_from_not_ok_status\n",
      "    raise core._status_to_exception(e) from None  # pylint: disable=protected-access\n",
      "tensorflow.python.framework.errors_impl.InvalidArgumentError: {{function_node __wrapped__AddV2_device_/job:localhost/replica:0/task:0/device:CPU:0}} Incompatible shapes: [32,2] vs. [13,256] [Op:AddV2] name: \n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/v0/t5xgy88d26bctff9sd0szwc80000gn/T/ipykernel_23656/3337422178.py\", line 31, in run_full_process\n",
      "    final_metrics = pipeline.train()\n",
      "  File \"/var/folders/v0/t5xgy88d26bctff9sd0szwc80000gn/T/ipykernel_23656/1212352700.py\", line 55, in train\n",
      "    round_metrics = self.fl_system.train_round(distributed_data)\n",
      "  File \"/var/folders/v0/t5xgy88d26bctff9sd0szwc80000gn/T/ipykernel_23656/4016177540.py\", line 119, in train_round\n",
      "    self._aggregate_models()\n",
      "  File \"/var/folders/v0/t5xgy88d26bctff9sd0szwc80000gn/T/ipykernel_23656/4016177540.py\", line 158, in _aggregate_models\n",
      "    aggregated_weights[f'actor_{var.name}'] += var / num_agents\n",
      "  File \"/Users/macbook/venv/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py\", line 153, in error_handler\n",
      "    raise e.with_traceback(filtered_tb) from None\n",
      "  File \"/Users/macbook/venv/lib/python3.9/site-packages/tensorflow/python/framework/ops.py\", line 6006, in raise_from_not_ok_status\n",
      "    raise core._status_to_exception(e) from None  # pylint: disable=protected-access\n",
      "tensorflow.python.framework.errors_impl.InvalidArgumentError: {{function_node __wrapped__AddV2_device_/job:localhost/replica:0/task:0/device:CPU:0}} Incompatible shapes: [32,2] vs. [13,256] [Op:AddV2] name: \n"
     ]
    }
   ],
   "source": [
    "# Bạn có thể bỏ comment phần này để chạy toàn bộ quy trình nếu mọi thứ đã sẵn sàng\n",
    "# Đảm bảo rằng bạn đã chạy tất cả các cell trước đó\n",
    "\n",
    "def run_full_process():\n",
    "    try:\n",
    "        print(\"Starting DDoS Detection with Federated Learning and PPO...\")\n",
    "        \n",
    "        # Step 1: Validate config\n",
    "        # config đã được khởi tạo ở cell 2\n",
    "        print(f\"Data path: {config.DATA_PATH}\")\n",
    "        print(f\"Selected features: {len(config.SELECTED_FEATURES)} features\")\n",
    "        print(f\"Label column: {config.LABEL_COLUMN}\")\n",
    "        \n",
    "        # Step 2: Preprocess data\n",
    "        preprocessor = DataPreprocessor(config)\n",
    "        X_train, X_test, y_train, y_test = preprocessor.preprocess_data()\n",
    "        \n",
    "        print(f\"Training data shape: {X_train.shape}\")\n",
    "        print(f\"Testing data shape: {X_test.shape}\")\n",
    "        print(f\"Class distribution in training: {np.bincount(y_train)}\")\n",
    "        print(f\"Class distribution in testing: {np.bincount(y_test)}\")\n",
    "        \n",
    "        # Step 3: Initialize federated learning system\n",
    "        fl_system = FederatedLearning(config)\n",
    "        \n",
    "        # Step 4: Initialize training pipeline\n",
    "        pipeline = TrainingPipeline(config, fl_system, X_train, y_train, X_test, y_test)\n",
    "        \n",
    "        # Step 5: Start training\n",
    "        print(\"Starting federated training...\")\n",
    "        final_metrics = pipeline.train()\n",
    "        \n",
    "        # Step 6: Analyze results\n",
    "        analyzer = ResultsAnalyzer(config)\n",
    "        analyzer.plot_training_history(pipeline.metrics_history)\n",
    "        analyzer.plot_confusion_matrix(y_test, final_metrics['predictions'])\n",
    "        analyzer.analyze_detection_performance(y_test, final_metrics['predictions'])\n",
    "        \n",
    "        print(\"Training completed successfully!\")\n",
    "        print(\"\\nFinal Metrics:\")\n",
    "        for k, v in final_metrics.items():\n",
    "            if k != 'predictions':\n",
    "                print(f\"{k}: {v:.4f}\")\n",
    "                \n",
    "        return final_metrics\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during execution: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "# Để chạy toàn bộ quy trình, uncomment dòng dưới\n",
    "# Lưu ý rằng quá trình này có thể tốn nhiều thời gian và tài nguyên\n",
    "\n",
    "final_metrics = run_full_process()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
